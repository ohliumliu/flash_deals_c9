{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"jdbc:mysql://localhost:3306/flashdeals\"\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read a table from database.\n",
    "\n",
    "product_clusters_df = spark.read.format(\"jdbc\").options(\n",
    "url = url,\n",
    "driver=\"com.mysql.jdbc.Driver\",\n",
    "dbtable=\"product_clusters\",\n",
    "user=\"root\",\n",
    "password=\"\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+--------------------+--------------------+\n",
      "| id|product_id|cluster|          created_at|          updated_at|\n",
      "+---+----------+-------+--------------------+--------------------+\n",
      "|  2|         2|      1|2016-11-21 06:05:...|2016-11-21 06:05:...|\n",
      "|  3|         2|      1|2016-11-21 06:05:...|2016-11-21 06:05:...|\n",
      "|  4|         2|      1|2016-11-21 06:05:...|2016-11-21 06:05:...|\n",
      "|  6|      null|      1|2016-11-21 06:05:...|2016-11-21 06:05:...|\n",
      "|  7|      null|      1|2016-11-21 07:04:...|2016-11-21 07:04:...|\n",
      "+---+----------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_clusters_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read a table from database.\n",
    "\n",
    "products_df = sqlContext.read.format(\"jdbc\").options(\n",
    "url = url,\n",
    "driver=\"com.mysql.jdbc.Driver\",\n",
    "dbtable=\"products\",\n",
    "user=\"root\",\n",
    "password=\"\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read a table from database.\n",
    "\n",
    "product_cluster_df = sqlContext.read.format(\"jdbc\").options(\n",
    "url = url,\n",
    "driver=\"com.mysql.jdbc.Driver\",\n",
    "dbtable=\"product_clusters\",\n",
    "user=\"root\",\n",
    "password=\"\"\n",
    ").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('name', 'string'),\n",
       " ('description', 'string'),\n",
       " ('created_at', 'timestamp'),\n",
       " ('updated_at', 'timestamp'),\n",
       " ('detail_page_url', 'string'),\n",
       " ('manufacturer', 'string'),\n",
       " ('list_price', 'int'),\n",
       " ('title', 'string'),\n",
       " ('small_image_url', 'string'),\n",
       " ('medium_image_url', 'string'),\n",
       " ('price', 'int'),\n",
       " ('amount_saved', 'int'),\n",
       " ('percentage_saved', 'int'),\n",
       " ('is_supersaver_shipping', 'boolean'),\n",
       " ('is_prime', 'boolean'),\n",
       " ('ASIN', 'string'),\n",
       " ('merchant_id', 'int'),\n",
       " ('catalog_id', 'int'),\n",
       " ('dealer_id', 'int')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'int'),\n",
       " ('product_id', 'int'),\n",
       " ('cluster', 'int'),\n",
       " ('created_at', 'timestamp'),\n",
       " ('updated_at', 'timestamp')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cluster_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, product_id=1, cluster=0, created_at=datetime.datetime(2016, 11, 21, 6, 5, 32), updated_at=datetime.datetime(2016, 11, 21, 6, 5, 32))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cluster_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|  id|catalog_id|\n",
      "+----+----------+\n",
      "|2245|        19|\n",
      "|2246|        19|\n",
      "|2247|        19|\n",
      "|2248|        19|\n",
      "|2249|        19|\n",
      "|2250|        19|\n",
      "|2251|        19|\n",
      "|2252|        19|\n",
      "|2253|        19|\n",
      "|2264|        19|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple operation\n",
    "categories_df = products_df.select([\"id\", \"catalog_id\"])\n",
    "categories_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, product_id: int, cluster: int, created_at: timestamp, updated_at: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(url=url, table=\"product_clusters\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to read. see readme for pyspark options for this to work.\n",
    "products_df = sqlContext.read.jdbc(url = url, table=\"products\", properties=properties)\n",
    "products_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save categories_df as a new table in db\n",
    "categories_df.select('id').write.jdbc(url=url, table=\"categories\", mode = \"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overwrite mode works without error, but it doesn't save anything to the table.\\\n",
    "# if using append mode, id needs to be unique to avoid errors. Still, nothing was saved to the table.\n",
    "product_cluster_df.select('*').write.jdbc(url=url, table=\"product_clusters\", mode = \"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this works. need to satisfy two things at least:\n",
    "# * id is unique\n",
    "# * schema match with non-nullable data filled\n",
    "from pyspark.sql import Row\n",
    "from datetime import date, datetime\n",
    "new_cluster_df = sqlContext.createDataFrame([Row(id=9, product_id=2, cluster=1, created_at=datetime(2016, 11, 21, 6, 5, 32), updated_at=datetime(2016, 11, 21, 6, 5, 32))])\n",
    "new_cluster_df.write.jdbc(url=url, table=\"product_clusters\", mode = \"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_cluster_df.agg({\"id\": \"max\"}).collect()[0]['max(id)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this works\n",
    "from pyspark.sql import Row\n",
    "from datetime import date, datetime\n",
    "new_id = product_cluster_df.agg({\"id\": \"max\"}).collect()[0]['max(id)'] + 1\n",
    "new_cluster_df = sqlContext.createDataFrame([Row(id=new_id, product_id=5, cluster=1, created_at=datetime.now(), updated_at=datetime.now())])\n",
    "new_cluster_df.write.jdbc(url=url, table=\"product_clusters\", mode = \"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pipeline\n",
    "\n",
    "* Tokenizer of lower case\n",
    "* StopWordsRemover\n",
    "* HashingTF\n",
    "* IDF\n",
    "* Normalizer\n",
    "* kmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|         title_words|\n",
      "+--------------------+--------------------+\n",
      "|InnoGear® 100ml A...|[innogear®, 100ml...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "words_df = tokenizer.transform(products_df)\n",
    "words_df.select([\"title\", \"title_words\"]).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                                                                                                              |title_words                                                                                                                                                                                                                    |title_words_filtered                                                                                                                                                                                           |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|InnoGear® 100ml Aromatherapy Essential Oil Diffuser Portable Ultrasonic Cool Mist Aroma Humidifier with Color LED Lights Changing and Waterless Auto Shut-off Function for Home Office Bedroom Room|[innogear®, 100ml, aromatherapy, essential, oil, diffuser, portable, ultrasonic, cool, mist, aroma, humidifier, with, color, led, lights, changing, and, waterless, auto, shut-off, function, for, home, office, bedroom, room]|[innogear®, 100ml, aromatherapy, essential, oil, diffuser, portable, ultrasonic, cool, mist, aroma, humidifier, color, led, lights, changing, waterless, auto, shut-off, function, home, office, bedroom, room]|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopwords = StopWordsRemover(inputCol=\"title_words\", outputCol=\"title_words_filtered\")\n",
    "words_filter_df = stopwords.transform(words_df)\n",
    "words_filter_df.select([\"title\", \"title_words\", \"title_words_filtered\"]).show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title_tf                                                                    |title_tf_idf                                                                                                                                                                                                                                    |\n",
      "+----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(16,[0,2,3,4,5,6,7,8,9,14,15],[2.0,1.0,4.0,1.0,1.0,4.0,1.0,3.0,1.0,2.0,4.0])|(16,[0,2,3,4,5,6,7,8,9,14,15],[1.6112503279732713,0.7591051483517427,3.036420593406971,0.5920510636885765,0.7591051483517427,2.6883750854484516,0.7591051483517427,1.7761531910657296,0.9057086225436182,1.7088306563121352,1.6646415888996493])|\n",
      "|(16,[1,4,5,8,9,11,12,13,15],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0])          |(16,[1,4,5,8,9,11,12,13,15],[0.7591051483517427,0.5920510636885765,0.7591051483517427,0.5920510636885765,0.9057086225436182,0.5179430915348546,1.9195516876277878,1.0169342576538425,0.4161603972249123])                                       |\n",
      "|(16,[1,6,9,12,13,14],[2.0,1.0,2.0,1.0,1.0,1.0])                             |(16,[1,6,9,12,13,14],[1.5182102967034854,0.6720937713621129,1.8114172450872363,0.9597758438138939,1.0169342576538425,0.8544153281560676])                                                                                                       |\n",
      "+----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, CountVectorizer\n",
    "\n",
    "tf = HashingTF(inputCol=\"title_words_filtered\", outputCol=\"title_tf\", numFeatures=16)\n",
    "words_filter_tf_df = tf.transform(words_filter_df)\n",
    "words_filter_tf_df.count()\n",
    "\n",
    "# Can also use CountVectorizer which allows easier inspection of the bag of words.\n",
    "\n",
    "idf = IDF(inputCol=\"title_tf\", outputCol=\"title_tf_idf\")\n",
    "words_filter_tf_idf_df = idf.fit(words_filter_tf_df).transform(words_filter_tf_df)\n",
    "words_filter_tf_idf_df.select([\"title_tf\", \"title_tf_idf\"]).show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title_tf_idf                                                                                                                                                                                                                                    |title_tf_idf_norm                                                                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(16,[0,2,3,4,5,6,7,8,9,14,15],[1.6112503279732713,0.7591051483517427,3.036420593406971,0.5920510636885765,0.7591051483517427,2.6883750854484516,0.7591051483517427,1.7761531910657296,0.9057086225436182,1.7088306563121352,1.6646415888996493])|(16,[0,2,3,4,5,6,7,8,9,14,15],[0.290380209052455,0.1368062478214934,0.5472249912859736,0.10669969070533472,0.1368062478214934,0.48450008404707856,0.1368062478214934,0.32009907211600414,0.16322718735185246,0.30796617670162124,0.3000024044619525])|\n",
      "|(16,[1,4,5,8,9,11,12,13,15],[0.7591051483517427,0.5920510636885765,0.7591051483517427,0.5920510636885765,0.9057086225436182,0.5179430915348546,1.9195516876277878,1.0169342576538425,0.4161603972249123])                                       |(16,[1,4,5,8,9,11,12,13,15],[0.27121058739629395,0.211526054197141,0.27121058739629395,0.211526054197141,0.32358859383749433,0.18504900197031826,0.6858111051801886,0.36332692244357584,0.14868441615435446])                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"title_tf_idf\", outputCol=\"title_tf_idf_norm\")\n",
    "words_filter_tf_idf_normalize_df = normalizer.transform(words_filter_tf_idf_df)\n",
    "words_filter_tf_idf_normalize_df.select([\"title_tf_idf\", \"title_tf_idf_norm\"]).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   avg(prediction)|\n",
      "+------------------+\n",
      "|0.3076923076923077|\n",
      "|              0.65|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"title_tf_idf_norm\", k=2, seed=20)\n",
    "model = kmeans.fit(words_filter_tf_idf_normalize_df)\n",
    "words_kmeans_df = model.transform(words_filter_tf_idf_normalize_df)\n",
    "words_kmeans_df.groupBy('catalog_id').avg().select(\"avg(prediction)\").show(46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('kmeans_model_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeansModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2450ece63788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeansModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kmeans_model_only'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeansModel' is not defined"
     ]
    }
   ],
   "source": [
    "load_kmeans = KMeansModel.load('kmeans_model_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|catalog_id|prediction|\n",
      "+----------+----------+\n",
      "|        19|         0|\n",
      "|        19|         1|\n",
      "|        19|         1|\n",
      "|        19|         0|\n",
      "|        19|         1|\n",
      "|        19|         1|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        21|         0|\n",
      "|        21|         1|\n",
      "|        21|         0|\n",
      "|        21|         0|\n",
      "|        21|         1|\n",
      "|        21|         1|\n",
      "|        21|         1|\n",
      "|        21|         0|\n",
      "|        21|         1|\n",
      "|        21|         0|\n",
      "|        19|         0|\n",
      "|        21|         1|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        21|         1|\n",
      "|        21|         0|\n",
      "|        19|         1|\n",
      "|        21|         0|\n",
      "|        19|         1|\n",
      "|        21|         1|\n",
      "|        19|         0|\n",
      "|        19|         1|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        21|         1|\n",
      "|        21|         1|\n",
      "|        21|         1|\n",
      "|        21|         1|\n",
      "|        19|         0|\n",
      "|        19|         1|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        19|         0|\n",
      "|        21|         1|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_kmeans_df.select([\"catalog_id\", \"prediction\"]).show(46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the whole process in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "\n",
    "step = 0 \n",
    "\n",
    "step += 1\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=str(step) + \"_tokenizer\")\n",
    "\n",
    "step += 1\n",
    "stopwords = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=str(step) + \"_stopwords\")\n",
    "\n",
    "step += 1\n",
    "tf = HashingTF(inputCol=stopwords.getOutputCol(), outputCol=str(step) + \"_tf\", numFeatures=160)\n",
    "\n",
    "step += 1\n",
    "idf = IDF(inputCol=tf.getOutputCol(), outputCol=str(step) + \"_idf\")\n",
    "\n",
    "step += 1\n",
    "normalizer = Normalizer(inputCol=idf.getOutputCol(), outputCol=str(step) + \"_normalizer\")\n",
    "\n",
    "step += 1\n",
    "kmeans = KMeans(featuresCol=normalizer.getOutputCol(), predictionCol = str(step) + \"_kmeans\", k=2, seed=20)\n",
    "\n",
    "kmeans_pipeline = Pipeline(stages=[tokenizer, stopwords, tf, idf, normalizer, kmeans])\n",
    "\n",
    "model = kmeans_pipeline.fit(products_df)\n",
    "words_prediction = model.transform(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|      avg(6_KMeans)|\n",
      "+-------------------+\n",
      "|0.19230769230769232|\n",
      "|               0.05|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_prediction.groupBy('catalog_id').avg().select(\"avg(\" + stages[-1].getPredictionCol() + \")\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_sort_cols',\n",
       " 'agg',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'coalesce',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'explain',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'intersect',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'na',\n",
       " 'orderBy',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'subtract',\n",
       " 'take',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unpersist',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'write',\n",
       " 'writeStream']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to write the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature = \"title\"\n",
    "stages=[Tokenizer(), StopWordsRemover(), HashingTF(numFeatures=32), IDF(),\n",
    "                                   Normalizer(), KMeans(k = 2, seed = 20)]\n",
    "stages[0].setInputCol(feature)\n",
    "stages[0].setOutputCol(\"1_\"+stages[0].__class__.__name__)\n",
    "\n",
    "for (i, s) in enumerate(stages[1:-1]):\n",
    "    i = i + 1\n",
    "    s.setInputCol(stages[i-1].getOutputCol())\n",
    "    s.setOutputCol(str(i+1)+\"_\"+s.__class__.__name__)\n",
    "\n",
    "stages[-1].setFeaturesCol(stages[-2].getOutputCol())\n",
    "stages[-1].setPredictionCol(str(len(stages)) + \"_\" + stages[-1].__class__.__name__)\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(products_df)\n",
    "prediction = model.transform(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_pipeline = pyspark.ml.PipelineModel.load('kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|6_kmeans|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       1|\n",
      "|       0|\n",
      "|       1|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_pipeline.transform(products_df).select(\"6_kmeans\").show(5) # test loaded pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|6_kmeans|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       0|\n",
      "|       0|\n",
      "|       1|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_pipeline.transform(products_df.filter(products_df.catalog_id == 21)).select(\"6_kmeans\").show(5) # fit only part of the products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|6_kmeans|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit with any input with a title field\n",
    "test_df = sqlContext.createDataFrame([{'title': 'a b c'}])\n",
    "loaded_pipeline.transform(test_df).select(\"6_kmeans\").show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(title=u'a b c')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     avg(6_KMeans)|\n",
      "+------------------+\n",
      "|0.3076923076923077|\n",
      "|              0.65|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.groupBy('catalog_id').avg().select(\"avg(\" + stages[-1].getPredictionCol() + \")\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(lp):\n",
    "    uid = lp[lp.find('(') + 1: lp.find(')')]\n",
    "    title = lp[lp.find('[') + 1: lp.find(']')]\n",
    "    return uid, title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_title = parse('(333) [a b c]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b c'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_title[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
